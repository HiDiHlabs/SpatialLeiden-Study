{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9453d41c-c85a-4919-8b7b-eb59860619d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import anndata as ad\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import squidpy as sq\n",
    "from scipy.sparse import find\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a44293d-7e68-431d-8114-0f1274cc57bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"./data\")\n",
    "result_dir = Path(\"./results\")\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c4d5f5a-0931-425f-82e3-dc6098c7d19b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata = pd.read_table(\n",
    "    data_dir / \"samples.tsv\", usecols=[\"directory\", \"n_clusters\"]\n",
    ").set_index(\"directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "782c824c-ad40-485f-8a8f-f2941fd883d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anndata(path):\n",
    "    X = sp.io.mmread(path / \"counts.mtx\").tocsr()\n",
    "\n",
    "    observations = pd.read_table(path / \"observations.tsv\", index_col=0)\n",
    "    features = pd.read_table(path / \"features.tsv\", index_col=0)\n",
    "\n",
    "    coordinates = (\n",
    "        pd.read_table(path / \"coordinates.tsv\", index_col=0)\n",
    "        .loc[observations.index, :]\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "    adata = ad.AnnData(\n",
    "        X=X, obs=observations, var=features, obsm={\"spatial\": coordinates}\n",
    "    )\n",
    "\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f5079c-0e32-451d-90da-b0c395acdaf0",
   "metadata": {},
   "source": [
    "# Leiden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77b53e60-3d03-411c-a079-defa6a179212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_resolution(\n",
    "    adata, ncluster, start=1, step=0.1, n_iterations=15, seed=42, **kwargs\n",
    "):\n",
    "    # adapted from SpaGCN.search_res (https://github.com/jianhuupenn/SpaGCN)\n",
    "    res = start\n",
    "    sc.tl.leiden(adata, resolution=res, random_state=seed, **kwargs)\n",
    "    old_ncluster = adata.obs[\"leiden\"].cat.categories.size\n",
    "    iter = 1\n",
    "    while old_ncluster != ncluster:\n",
    "        old_sign = 1 if (old_ncluster < ncluster) else -1\n",
    "        sc.tl.leiden(\n",
    "            adata, resolution=res + step * old_sign, random_state=seed, **kwargs\n",
    "        )\n",
    "        new_ncluster = adata.obs[\"leiden\"].cat.categories.size\n",
    "        if new_ncluster == ncluster:\n",
    "            res = res + step * old_sign\n",
    "            # print(f\"Recommended res = {res:.2f}\")\n",
    "            return res\n",
    "        new_sign = 1 if (new_ncluster < ncluster) else -1\n",
    "        if new_sign == old_sign:\n",
    "            res = res + step * old_sign\n",
    "            # print(f\"Res changed to {res:.2f}\")\n",
    "            old_ncluster = new_ncluster\n",
    "        else:\n",
    "            step = step / 2\n",
    "            # print(f\"Step changed to {step:.2f}\")\n",
    "        if iter > n_iterations:\n",
    "            # print(\"Exact resolution not found\")\n",
    "            # print(f\"Recommended res =  {res:.2f}\")\n",
    "            return res\n",
    "        iter += 1\n",
    "    # print(f\"Recommended res = {res:.2f}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c843ae7c-345b-4e71-bfc7-fc7552df7830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_adata(adata, seed=42, genes=1000, n_pcs=30):\n",
    "    sc.pp.filter_genes(adata, min_cells=3)\n",
    "    sc.pp.highly_variable_genes(adata, flavor=\"seurat_v3\", n_top_genes=genes)\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    sc.tl.pca(adata, n_comps=n_pcs, random_state=seed)\n",
    "    sc.pp.neighbors(adata, random_state=seed)\n",
    "\n",
    "\n",
    "def run_leiden(adata, n_clusters, seed, **kwargs):\n",
    "    res = search_resolution(adata, n_clusters, seed=seed, **kwargs)\n",
    "\n",
    "    df = adata.obs[[\"leiden\"]]\n",
    "    df.columns = [\"label\"]\n",
    "    return df, res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c4b89d-1589-445f-b186-96bc4209bd9c",
   "metadata": {},
   "source": [
    "## Multiplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a08aed9-dc59-47fe-ad2f-acf382b0aff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_igraph(adjacency, directed=True):\n",
    "    # adapted from scanpy\n",
    "    sources, targets, weights = find(adjacency)\n",
    "    g = ig.Graph(directed=directed)\n",
    "    g.add_vertices(adjacency.shape[0])\n",
    "    g.add_edges(list(zip(sources, targets)))\n",
    "    g.es[\"weight\"] = weights\n",
    "    if g.vcount() != adjacency.shape[0]:\n",
    "        raise RuntimeError(\n",
    "            f\"The constructed graph has only {g.vcount()} nodes. \"\n",
    "            \"Your adjacency matrix contained redundant nodes.\"\n",
    "        )\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a14cfa62-7db8-4ed8-b0fa-bf6752bdfef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_spatial_resolution_multiplex(\n",
    "    adata, ncluster, start=0.4, step=0.1, n_iterations=15, seed=42, **kwargs\n",
    "):\n",
    "    # adapted from SpaGCN.search_res (https://github.com/jianhuupenn/SpaGCN)\n",
    "    res = start\n",
    "    spatial_partition_kwargs = kwargs.pop(\"spatial_partition_kwargs\", dict())\n",
    "    spatial_partition_kwargs[\"resolution_parameter\"] = res\n",
    "    leiden_multiplex(\n",
    "        adata, spatial_partition_kwargs=spatial_partition_kwargs, seed=seed, **kwargs\n",
    "    )\n",
    "    old_ncluster = adata.obs[\"leiden_multiplex\"].cat.categories.size\n",
    "    iter = 1\n",
    "    while old_ncluster != ncluster:\n",
    "        old_sign = 1 if (old_ncluster < ncluster) else -1\n",
    "        spatial_partition_kwargs[\"resolution_parameter\"] = res + step * old_sign\n",
    "        leiden_multiplex(\n",
    "            adata,\n",
    "            spatial_partition_kwargs=spatial_partition_kwargs,\n",
    "            seed=seed,\n",
    "            **kwargs,\n",
    "        )\n",
    "        new_ncluster = adata.obs[\"leiden_multiplex\"].cat.categories.size\n",
    "        if new_ncluster == ncluster:\n",
    "            res = res + step * old_sign\n",
    "            # print(f\"Recommended res = {res:.2f}\")\n",
    "            return res\n",
    "        new_sign = 1 if (new_ncluster < ncluster) else -1\n",
    "        if new_sign == old_sign:\n",
    "            res = res + step * old_sign\n",
    "            # print(f\"Res changed to {res:.2f}\")\n",
    "            old_ncluster = new_ncluster\n",
    "        else:\n",
    "            step = step / 2\n",
    "            # print(f\"Step changed to {step:.2f}\")\n",
    "        if iter > n_iterations:\n",
    "            # print(\"Exact resolution not found\")\n",
    "            # print(f\"Recommended res = {res:.2f}\")\n",
    "            return res\n",
    "        iter += 1\n",
    "    # print(f\"Recommended res = {res:.2f}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eb6ed4d-feea-4391-b491-39cace7cc638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leiden_multiplex(\n",
    "    adata,\n",
    "    key_added: str = \"leiden_multiplex\",\n",
    "    directed: tuple[bool, bool] = (True, True),\n",
    "    use_weights: bool = True,\n",
    "    n_iterations: int = -1,\n",
    "    partition_type=la.RBConfigurationVertexPartition,\n",
    "    scale_graph_weights: tuple[bool, bool] = (False, False),\n",
    "    layer_weights: tuple[int, int] = (1, 1),\n",
    "    spatial_partition_kwargs=None,\n",
    "    latent_partition_kwargs=None,\n",
    "    diff_threshold: float = 1e-05,\n",
    "    seed=42,\n",
    "):\n",
    "    spatial_distance_key = \"spatial_distances\"\n",
    "    latent_distance_key = \"connectivities\"\n",
    "\n",
    "    latent_distances = adata.obsp[latent_distance_key]\n",
    "    spatial_distances = adata.obsp[spatial_distance_key]\n",
    "\n",
    "    if scale_graph_weights[0]:\n",
    "        percentile = np.percentile(latent_distances.data, 95)\n",
    "        latent_distances = latent_distances.multiply(1 / percentile)\n",
    "    if scale_graph_weights[1]:\n",
    "        percentile = np.percentile(spatial_distances.data, 95)\n",
    "        spatial_distances = spatial_distances.multiply(1 / percentile)\n",
    "\n",
    "    adjacency_latent = _build_igraph(latent_distances, directed=directed[0])\n",
    "    adjacency_spatial = _build_igraph(spatial_distances, directed=directed[1])\n",
    "\n",
    "    # parameterise the partitions\n",
    "    if spatial_partition_kwargs is None:\n",
    "        spatial_partition_kwargs = dict()\n",
    "    if latent_partition_kwargs is None:\n",
    "        latent_partition_kwargs = dict()\n",
    "\n",
    "    if use_weights:\n",
    "        spatial_partition_kwargs[\"weights\"] = \"weight\"\n",
    "        latent_partition_kwargs[\"weights\"] = \"weight\"\n",
    "\n",
    "    latent_part = partition_type(adjacency_latent, **latent_partition_kwargs)\n",
    "    spatial_part = partition_type(adjacency_spatial, **spatial_partition_kwargs)\n",
    "    optimiser = la.Optimiser()\n",
    "    optimiser.set_rng_seed(seed)\n",
    "\n",
    "    diff = optimiser.optimise_partition_multiplex(\n",
    "        [latent_part, spatial_part],\n",
    "        layer_weights=list(layer_weights),\n",
    "        n_iterations=n_iterations,\n",
    "    )\n",
    "\n",
    "    adata.obs[key_added] = np.array(latent_part.membership)\n",
    "    adata.obs[key_added] = adata.obs[key_added].astype(\"category\")\n",
    "\n",
    "\n",
    "def run_leiden_multiplex(adata, n_clusters, seed, **kwargs):\n",
    "    res = search_spatial_resolution_multiplex(adata, n_clusters, seed=seed, **kwargs)\n",
    "\n",
    "    df = adata.obs[[\"leiden_multiplex\"]]\n",
    "    df.columns = [\"label\"]\n",
    "    return df, res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bdf794-e277-4176-babf-aee78a2aa6ad",
   "metadata": {},
   "source": [
    "# MULTISPATI-PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdf0ada7-a64b-461a-8213-0f98f9af6fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import TypeAlias, TypeVar\n",
    "\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from scipy import linalg\n",
    "from scipy.sparse import csc_array, csc_matrix, csr_array, csr_matrix, issparse\n",
    "from scipy.sparse import linalg as sparse_linalg\n",
    "from scipy.sparse import sparray, spmatrix\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "T = TypeVar(\"T\", bound=np.number)\n",
    "U = TypeVar(\"U\", bound=np.number)\n",
    "\n",
    "\n",
    "_Csr: TypeAlias = csr_array | csr_matrix\n",
    "_Csc: TypeAlias = csc_array | csc_matrix\n",
    "_X: TypeAlias = np.ndarray | _Csr | _Csc\n",
    "\n",
    "\n",
    "class MultispatiPCA:\n",
    "    \"\"\"\n",
    "    MULTISPATI-PCA (Principal Component Analysis).\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO, should scaling be part of multispati\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_components: int | tuple[int, int] | None = None,\n",
    "        *,\n",
    "        connectivity: sparray | spmatrix,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        n_components : int or tuple[int, int], optional\n",
    "            Number of components to keep.\n",
    "            If None, will keep all components (only supported for non-sparse X).\n",
    "            If an int, it will keep the top `n_components`.\n",
    "            If a tuple, it will keep the top and bottom `n_components` respectively.\n",
    "        connectivity : scipy.sparse.sparray or scipy.sparse.spmatrix\n",
    "            Matrix of row-wise neighbor definitions i.e. c_ij is the connectivity of\n",
    "            i -> j. Does not have to be symmetric. Can be a binary adjacency matrix\n",
    "            or a matrix of connectivities in which case c_ij should be larger\n",
    "            if i and j are close.\n",
    "            A distance matrix should transformed to connectivities by e.g.\n",
    "            calculating :math:`1-d/d_{max}` beforehand.\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If connectivity is not a square matrix.\n",
    "        ZeroDivisionError\n",
    "            If one of the observations has no neighbors.\n",
    "        \"\"\"\n",
    "        self._fitted = False\n",
    "        W = csr_array(connectivity)\n",
    "        if W.shape[0] != W.shape[1]:\n",
    "            raise ValueError(\"`connectivity` must be square\")\n",
    "        self.W = self._normalize_connectivities(W)\n",
    "\n",
    "        n = self.W.shape[0]\n",
    "\n",
    "        self._n_neg = 0\n",
    "        if n_components is None:\n",
    "            self._n_pos = n_components\n",
    "        else:\n",
    "            if isinstance(n_components, int):\n",
    "                if n_components > n:\n",
    "                    warnings.warn(\n",
    "                        \"`n_components` should be less or equal than \"\n",
    "                        f\"#rows of `connectivity`. Using {n} components.\"\n",
    "                    )\n",
    "                self._n_pos = min(n_components, n)\n",
    "            elif isinstance(n_components, tuple) and len(n_components) == 2:\n",
    "                if n < n_components[0] + n_components[1]:\n",
    "                    warnings.warn(\n",
    "                        \"Sum of `n_components` should be less or equal than \"\n",
    "                        f\"#rows of `connectivity`. Using {n} components.\"\n",
    "                    )\n",
    "                    self._n_pos = n\n",
    "                else:\n",
    "                    self._n_pos, self._n_neg = n_components\n",
    "            else:\n",
    "                raise ValueError(\"`n_components` must be None, int or (int, int)\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_connectivities(W: csr_array) -> csr_array:\n",
    "        # normalize rowsums to 1 for better interpretability\n",
    "        # TODO can't handle points without neighbors because of division by zero\n",
    "        return W.multiply(1 / W.sum(axis=1)[:, np.newaxis])\n",
    "\n",
    "    def fit(self, X: _X):\n",
    "        \"\"\"\n",
    "        Fit MULTISPATI-PCA projection.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray or scipy.sparse.csr_array or scipy.sparse.csc_array\n",
    "            Array of observations x features.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If `X` has not the same number of rows like `connectivity`.\n",
    "            If `n_components` is None and `X` is sparse.\n",
    "            If (sum of) `n_components` is larger than the smaller dimension of `X`.\n",
    "        \"\"\"\n",
    "        if issparse(X):\n",
    "            X = csc_array(X)\n",
    "\n",
    "        assert isinstance(X, (np.ndarray, csc_array))\n",
    "        if X.shape[0] != self.W.shape[0]:\n",
    "            raise ValueError(\"#rows in `X` must be same as size of `connectivity`\")\n",
    "        if self._n_pos is None:\n",
    "            if issparse(X):\n",
    "                raise ValueError(\n",
    "                    \"`n_components` is None, but `X` is a sparse matrix. None is only \"\n",
    "                    \"supported for dense matrices.\"\n",
    "                )\n",
    "        elif (self._n_pos + self._n_neg) > X.shape[1]:\n",
    "            n, d = X.shape\n",
    "            n_comp = self._n_pos + self._n_neg\n",
    "            n_comp_max = min(n, d)\n",
    "            raise ValueError(\n",
    "                f\"Requested {n_comp} components but given `X` at most {n_comp_max} \"\n",
    "                \"can be calculated.\"\n",
    "            )\n",
    "\n",
    "        # Data must be scaled, avoid mean-centering for sparse\n",
    "        X = scale(X, with_mean=not issparse(X))\n",
    "\n",
    "        eig_val, eig_vec = self._multispati_eigendecomposition(X, self.W)\n",
    "\n",
    "        self.components_ = eig_vec\n",
    "        self.eigenvalues_ = eig_val\n",
    "        self._fitted = True\n",
    "\n",
    "    def _multispati_eigendecomposition(\n",
    "        self, X: _X, W: _Csr\n",
    "    ) -> tuple[NDArray[np.float64], NDArray[np.float64]]:\n",
    "        # X: beads/bin x gene, must be standardized\n",
    "        # W: row-wise definition of neighbors, row-sums should be 1\n",
    "        def remove_zero_eigenvalues(\n",
    "            eigen_values: NDArray[T], eigen_vectors: NDArray[U], n: int\n",
    "        ) -> tuple[NDArray[T], NDArray[U]]:\n",
    "            keep_idx = np.sort(np.argpartition(np.abs(eigen_values), -n)[-n:])\n",
    "\n",
    "            return eigen_values[keep_idx], eigen_vectors[:, keep_idx]\n",
    "\n",
    "        n, d = X.shape\n",
    "\n",
    "        H = (X.T @ (W + W.T) @ X) / (2 * n)\n",
    "        # TODO handle sparse based on density?\n",
    "        if issparse(H):\n",
    "            # TODO fix can't return all eigenvalues of sparse matrix\n",
    "            # TODO check that number of eigenvalues does not exceed d\n",
    "            if self._n_pos is None:\n",
    "                raise ValueError\n",
    "            elif self._n_pos == 0:\n",
    "                eig_val, eig_vec = sparse_linalg.eigsh(H, k=self._n_neg, which=\"SA\")\n",
    "            elif self._n_neg == 0:\n",
    "                eig_val, eig_vec = sparse_linalg.eigsh(H, k=self._n_pos, which=\"LA\")\n",
    "            else:\n",
    "                n_comp = 2 * max(self._n_neg, self._n_pos)\n",
    "                eig_val, eig_vec = sparse_linalg.eigsh(H, k=n_comp, which=\"BE\")\n",
    "                component_indices = self._get_component_indices(\n",
    "                    n_comp, self._n_pos, self._n_neg\n",
    "                )\n",
    "                eig_val = eig_val[component_indices]\n",
    "                eig_vec = eig_vec[:, component_indices]\n",
    "\n",
    "        else:\n",
    "            if self._n_pos is None:\n",
    "                eig_val, eig_vec = linalg.eigh(H)\n",
    "                if n < d:\n",
    "                    eig_val, eig_vec = remove_zero_eigenvalues(eig_val, eig_vec, n)\n",
    "            elif self._n_pos == 0:\n",
    "                eig_val, eig_vec = linalg.eigh(H, subset_by_index=[0, self._n_neg])\n",
    "            elif self._n_neg == 0:\n",
    "                eig_val, eig_vec = linalg.eigh(\n",
    "                    H, subset_by_index=[d - self._n_pos, d - 1]\n",
    "                )\n",
    "            else:\n",
    "                eig_val, eig_vec = linalg.eigh(H)\n",
    "                component_indices = self._get_component_indices(\n",
    "                    d, self._n_pos, self._n_neg\n",
    "                )\n",
    "                eig_val = eig_val[component_indices]\n",
    "                eig_vec = eig_vec[:, component_indices]\n",
    "\n",
    "        return np.flip(eig_val), np.fliplr(eig_vec)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_component_indices(n: int, n_pos: int, n_neg: int) -> list[int]:\n",
    "        if n_pos + n_neg > n:\n",
    "            return list(range(n))\n",
    "        else:\n",
    "            return list(range(n_neg)) + list(range(n - n_pos, n))\n",
    "\n",
    "    def transform(self, X: _X) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform the data using fitted MULTISPATI-PCA projection.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray or scipy.sparse.csr_array or scipy.sparse.csc_array\n",
    "            Array of observations x features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If instance has not been fitted.\n",
    "        \"\"\"\n",
    "        # Data must be scaled, avoid mean-centering for sparse\n",
    "        if not self._fitted:\n",
    "            self._not_fitted()\n",
    "\n",
    "        X = scale(X, with_mean=not issparse(X))\n",
    "        return X @ self.components_\n",
    "\n",
    "    def fit_transform(self, X: _X) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fit the MULTISPATI-PCA projection and transform the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray or scipy.sparse.csr_array or scipy.sparse.csc_array\n",
    "            Array of observations x features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def transform_spatial_lag(self, X: _X) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform the data using fitted MULTISPATI-PCA projection and calculate the\n",
    "        spatial lag.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray or scipy.sparse.csr_array or scipy.sparse.csc_array\n",
    "            Array of observations x features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If instance has not been fitted.\n",
    "        \"\"\"\n",
    "        if not self._fitted:\n",
    "            self._not_fitted()\n",
    "        return self.W @ self.transform(X)\n",
    "\n",
    "    def variance_moranI_decomposition(self, X: _X) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Calculate the decomposition of the variance and Moran's I for `n_components`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray or scipy.sparse.csr_array or scipy.sparse.csc_array\n",
    "            Array of observations x features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[numpy.ndarray, numpy.ndarray]\n",
    "            Variance and Moran's I.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If instance has not been fitted.\n",
    "        \"\"\"\n",
    "        if not self._fitted:\n",
    "            self._not_fitted()\n",
    "        transformed = self.transform(X)\n",
    "        lag = self.transform_spatial_lag(X)\n",
    "\n",
    "        # vector of row_Weights from dudi.PCA\n",
    "        # (we only use default row_weights i.e. 1/n anyways)\n",
    "        w = 1 / X.shape[0]\n",
    "\n",
    "        variance = np.sum(transformed * transformed * w, axis=0)\n",
    "        moran = np.sum(transformed * lag * w, axis=0) / variance\n",
    "\n",
    "        return variance, moran\n",
    "\n",
    "    def moransI_bounds(\n",
    "        self, *, sparse_approx: bool = True\n",
    "    ) -> tuple[float, float, float]:\n",
    "        \"\"\"\n",
    "        Calculate the minimum and maximum bound for Moran's I given the `connectivity`\n",
    "        and the expected value given the #observations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sparse_approx : bool\n",
    "            Only applicable if `connectivity` is sparse.\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[float, float, float]\n",
    "            Minimum bound, maximum bound, and expected value.\n",
    "        \"\"\"\n",
    "\n",
    "        # following R package adespatial::moran.bounds\n",
    "        # sparse approx is following adegenet sPCA as shown in screeplot/summary\n",
    "        def double_center(W):\n",
    "            if issparse(W):\n",
    "                W = W.toarray()\n",
    "\n",
    "            row_means = np.mean(W, axis=1, keepdims=True)\n",
    "            col_means = np.mean(W, axis=0, keepdims=True) - np.mean(row_means)\n",
    "\n",
    "            return W - row_means - col_means\n",
    "\n",
    "        # ensure symmetry\n",
    "        W = 0.5 * (self.W + self.W.T)\n",
    "\n",
    "        n_sample = W.shape[0]\n",
    "        s = n_sample / np.sum(W)  # 1 if original W has rowSums or colSums of 1\n",
    "\n",
    "        if not issparse(W) or not sparse_approx:\n",
    "            W = double_center(W)\n",
    "\n",
    "        if issparse(W):\n",
    "            eigen_values = s * sparse_linalg.eigsh(\n",
    "                W, k=2, which=\"BE\", return_eigenvectors=False\n",
    "            )\n",
    "        else:\n",
    "            eigen_values = s * linalg.eigvalsh(W, overwrite_a=True)\n",
    "\n",
    "        I_0 = -1 / (n_sample - 1)\n",
    "        I_min = min(eigen_values)\n",
    "        I_max = max(eigen_values)\n",
    "\n",
    "        return I_min, I_max, I_0\n",
    "\n",
    "    def _not_fitted(self):\n",
    "        raise ValueError(\n",
    "            \"This MultispatiPCA instance is not fitted yet. \"\n",
    "            \"Call 'fit' with appropriate arguments first.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02284a2-0ab2-4811-a0e6-51f377bad623",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90590e1f-9209-4b7b-ae71-b4ab2d4dc052",
   "metadata": {},
   "source": [
    "## Impact of layer weight ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48cf2d1a-be41-4408-942b-5a00b2551a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = metadata.loc[\"Br8100_151673\", :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "259fcec2-284d-4309-a00e-7e1c29ec65b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda_envs/scverse_3_10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_61478/3929316464.py:6: FutureWarning: In the future, the default backend for leiden will be igraph instead of leidenalg.\n",
      "\n",
      " To achieve the future defaults please pass: flavor=\"igraph\" and n_iterations=2.  directed must also be False to work with igraph's implementation.\n",
      "  sc.tl.leiden(adata, resolution=res, random_state=seed, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "n_genes = 3_000\n",
    "n_pcs = 30\n",
    "\n",
    "out_dir = result_dir / \"weightratio_impact\" / sample.name\n",
    "\n",
    "adata = get_anndata(data_dir / sample.name)\n",
    "preprocess_adata(adata, genes=n_genes, n_pcs=n_pcs, seed=seed)\n",
    "\n",
    "leiden_df, res = run_leiden(adata, sample.n_clusters, seed=seed)\n",
    "\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "leiden_df.to_csv(out_dir / \"leiden.tsv\", sep=\"\\t\", index_label=\"\")\n",
    "\n",
    "sq.gr.spatial_neighbors(adata, coord_type=\"grid\", n_neighs=6)\n",
    "\n",
    "for weight_ratio in [0, 0.2, 0.4, 0.6, 0.8, 1, 5, 10]:\n",
    "    multiplex_df, res_multi = run_leiden_multiplex(\n",
    "        adata,\n",
    "        sample.n_clusters,\n",
    "        directed=(False, False),\n",
    "        scale_graph_weights=(False, False),\n",
    "        layer_weights=(1, weight_ratio),\n",
    "        latent_partition_kwargs={\"resolution_parameter\": res},\n",
    "        seed=seed,\n",
    "    )\n",
    "    multiplex_df.to_csv(\n",
    "        out_dir / f\"spatial_leiden_w{weight_ratio:.1f}.tsv\", sep=\"\\t\", index_label=\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa180e-582c-455a-9c81-fd779f6ae012",
   "metadata": {},
   "source": [
    "## Cluster all samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e89dca-306d-4ad2-a770-db17999c6542",
   "metadata": {},
   "source": [
    "### HVGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1193af9c-f6be-45de-aab8-c7b3345271c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Br5292_151507\n",
      "Processing Br5292_151508\n",
      "Processing Br5292_151509\n",
      "Processing Br5292_151510\n",
      "Processing Br5595_151669\n",
      "Processing Br5595_151670\n",
      "Processing Br5595_151671\n",
      "Processing Br5595_151672\n",
      "Processing Br8100_151673\n",
      "Processing Br8100_151674\n",
      "Processing Br8100_151675\n",
      "Processing Br8100_151676\n"
     ]
    }
   ],
   "source": [
    "n_pcs = 30\n",
    "n_genes = 3_000\n",
    "weight_spatial = 0.8\n",
    "\n",
    "\n",
    "for name, sample in metadata.iterrows():\n",
    "    print(\"Processing \" + name)\n",
    "\n",
    "    sample_dir = data_dir / name\n",
    "    out_dir = result_dir / name\n",
    "\n",
    "    adata = get_anndata(sample_dir)\n",
    "    preprocess_adata(adata, genes=n_genes, n_pcs=n_pcs, seed=seed)\n",
    "\n",
    "    label_leiden, _ = run_leiden(adata, sample.n_clusters, seed=seed)\n",
    "\n",
    "    # Multiplex\n",
    "    sq.gr.spatial_neighbors(adata, coord_type=\"grid\", n_neighs=6)\n",
    "    label_leiden_multi, _ = run_leiden_multiplex(\n",
    "        adata,\n",
    "        sample.n_clusters,\n",
    "        directed=(False, False),\n",
    "        scale_graph_weights=(False, False),\n",
    "        layer_weights=(1, weight_spatial),\n",
    "        latent_partition_kwargs={\"resolution_parameter\": res},\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    ## Write output\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    label_leiden.to_csv(out_dir / \"leiden.tsv\", sep=\"\\t\", index_label=\"\")\n",
    "    label_leiden_multi.to_csv(out_dir / \"spatial_leiden.tsv\", sep=\"\\t\", index_label=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef43de3c-0f5a-4dff-be4f-9621f6310c73",
   "metadata": {},
   "source": [
    "### SVGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3986b147-7d6b-49e4-81a6-e668404b4f6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Br5292_151507\n",
      "Processing Br5292_151508\n",
      "Processing Br5292_151509\n",
      "Processing Br5292_151510\n",
      "Processing Br5595_151669\n",
      "Processing Br5595_151670\n",
      "Processing Br5595_151671\n",
      "Processing Br5595_151672\n",
      "Processing Br8100_151673\n",
      "Processing Br8100_151674\n",
      "Processing Br8100_151675\n",
      "Processing Br8100_151676\n"
     ]
    }
   ],
   "source": [
    "n_pcs = 30\n",
    "n_genes = 3_000\n",
    "weight_spatial = 0.8\n",
    "\n",
    "for name, sample in metadata.iterrows():\n",
    "    print(\"Processing \" + name)\n",
    "\n",
    "    sample_dir = data_dir / name\n",
    "    out_dir = result_dir / name\n",
    "\n",
    "    adata = get_anndata(sample_dir)\n",
    "    preprocess_adata(adata, genes=n_genes, n_pcs=n_pcs, seed=seed)\n",
    "    sq.gr.spatial_neighbors(adata, coord_type=\"grid\", n_neighs=6)\n",
    "    sq.gr.spatial_autocorr(adata, genes=adata.var_names, mode=\"moran\", seed=seed)\n",
    "    genes = adata.uns[\"moranI\"].nlargest(n_genes, columns=\"I\", keep=\"all\").index\n",
    "    adata.obsm[\"X_svg_pca\"] = sc.tl.pca(\n",
    "        adata[:, genes].X, n_comps=n_pcs, random_state=seed\n",
    "    )\n",
    "    sc.pp.neighbors(adata, use_rep=\"X_svg_pca\", random_state=seed)\n",
    "\n",
    "    label_leiden, _ = run_leiden(adata, sample.n_clusters, seed=seed)\n",
    "\n",
    "    # Multiplex\n",
    "    sq.gr.spatial_neighbors(adata, coord_type=\"grid\", n_neighs=6)\n",
    "    label_leiden_multi, _ = run_leiden_multiplex(\n",
    "        adata,\n",
    "        sample.n_clusters,\n",
    "        directed=(False, False),\n",
    "        scale_graph_weights=(False, False),\n",
    "        layer_weights=(1, weight_spatial),\n",
    "        latent_partition_kwargs={\"resolution_parameter\": res},\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    # Multiplex and MULTISPATI-PCA\n",
    "    adata.obsm[\"X_mspca\"] = MultispatiPCA(\n",
    "        30, connectivity=adata.obsp[\"connectivities\"]\n",
    "    ).fit_transform(adata[:, genes].X)\n",
    "    sc.pp.neighbors(adata, use_rep=\"X_mspca\", random_state=seed)\n",
    "\n",
    "    label_leiden_msPCA, res = run_leiden(adata, sample.n_clusters, seed=seed)\n",
    "    label_leiden_multi_msPCA, _ = run_leiden_multiplex(\n",
    "        adata,\n",
    "        sample.n_clusters,\n",
    "        directed=(False, False),\n",
    "        scale_graph_weights=(False, False),\n",
    "        layer_weights=(1, weight_spatial),\n",
    "        latent_partition_kwargs={\"resolution_parameter\": res},\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    ## Write output\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    label_leiden.to_csv(out_dir / \"leiden_svg.tsv\", sep=\"\\t\", index_label=\"\")\n",
    "    label_leiden_multi.to_csv(\n",
    "        out_dir / \"spatial_leiden_svg.tsv\", sep=\"\\t\", index_label=\"\"\n",
    "    )\n",
    "    label_leiden_msPCA.to_csv(\n",
    "        out_dir / \"leiden_svg_multispati.tsv\", sep=\"\\t\", index_label=\"\"\n",
    "    )\n",
    "    label_leiden_multi_msPCA.to_csv(\n",
    "        out_dir / \"spatial_leiden_svg_multispati.tsv\", sep=\"\\t\", index_label=\"\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scverse_3.10",
   "language": "python",
   "name": "scverse_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
